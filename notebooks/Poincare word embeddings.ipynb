{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-02-20T12:21:25.355414Z","iopub.status.busy":"2023-02-20T12:21:25.354735Z","iopub.status.idle":"2023-02-20T12:21:25.361142Z","shell.execute_reply":"2023-02-20T12:21:25.360191Z","shell.execute_reply.started":"2023-02-20T12:21:25.355343Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-02-20T12:34:45.634510Z","iopub.status.busy":"2023-02-20T12:34:45.633278Z","iopub.status.idle":"2023-02-20T12:34:47.289013Z","shell.execute_reply":"2023-02-20T12:34:47.287557Z","shell.execute_reply.started":"2023-02-20T12:34:45.634448Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","--2023-02-20 12:34:46--  https://raw.githubusercontent.com/facebookresearch/poincare-embeddings/624584cfbad684d74bf034f8dbacd515230bedc4/wordnet/mammals_filter.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 390 [text/plain]\n","Saving to: ‘mammals_filter.txt.6’\n","\n","mammals_filter.txt. 100%[===================>]     390  --.-KB/s    in 0s      \n","\n","2023-02-20 12:34:47 (18.0 MB/s) - ‘mammals_filter.txt.6’ saved [390/390]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/facebookresearch/poincare-embeddings/624584cfbad684d74bf034f8dbacd515230bedc4/wordnet/mammals_filter.txt\n","def get_mammal_relations():\n","    \"\"\"Gets mammal subtree by doing transitive closure.\n","    Taken from https://github.com/facebookresearch/poincare-embeddings.\n","    \"\"\"\n","    \n","    import re\n","    import pandas as pd\n","    import numpy as np\n","    from nltk.corpus import wordnet as wn\n","    from tqdm import tqdm \n","    import nltk\n","    nltk.download('wordnet')\n","    nltk.download('omw-1.4')\n","\n","\n","    \n","    edges = set()\n","    for synset in tqdm(wn.all_synsets(pos='n')):\n","        for hyper in synset.closure(lambda s: s.hypernyms()):\n","            edges.add((synset.name(), hyper.name()))\n","\n","        for instance in synset.instance_hyponyms():\n","            for hyper in instance.closure(lambda s: s.instance_hypernyms()):\n","                edges.add((instance.name(), hyper.name()))\n","                for h in hyper.closure(lambda s: s.hypernyms()):\n","                    edges.add((instance.name(), h.name()))\n","\n","    nouns = pd.DataFrame(list(edges), columns=['id1', 'id2'])\n","    nouns['weight'] = 1\n","    mammal_set = set(nouns[nouns.id2 == 'mammal.n.01'].id1.unique())\n","    mammal_set.add('mammal.n.01')\n","    mammals = nouns[nouns.id1.isin(mammal_set) & nouns.id2.isin(mammal_set)]\n","    with open('mammals_filter.txt', 'r') as fin:\n","        filt = re.compile(f'({\"|\".join([l.strip() for l in fin.readlines()])})')\n","    filtered_mammals = mammals[~mammals.id1.str.cat(' ' + mammals.id2).str.match(filt)]\n","\n","    nouns.to_csv('noun_closure.csv', index=False)\n","    filtered_mammals.to_csv('mammal_closure.csv', index=False)\n","    mammal = pd.read_csv('mammal_closure.csv')\n","    print('Total unique nodes: ', len(np.unique(list(mammal.id1.values) + list(mammal.id2.values))))\n","    mammal_relations = [[mammal.id1[i].split('.')[0], mammal.id2[i].split('.')[0]] for i in range(len(mammal))]\n","    return mammal_relations\n","    "]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-02-20T12:34:49.038072Z","iopub.status.busy":"2023-02-20T12:34:49.036837Z","iopub.status.idle":"2023-02-20T12:34:57.542002Z","shell.execute_reply":"2023-02-20T12:34:57.541032Z","shell.execute_reply.started":"2023-02-20T12:34:49.037992Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","82115it [00:05, 13799.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Total unique nodes:  1180\n","[['kid', 'even-toed_ungulate'], ['crowbait', 'ungulate'], ['addax', 'ungulate']]\n"]}],"source":["mammal_relations = get_mammal_relations()\n","print(mammal_relations[0:3])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = PoincareEmbed(num_embeddings=30, features=28)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from rieoptax.optimizers.rtx_train_state import RtxTrainState\n","\n","def create_train_state(module, rng, learning_rate, momentum):\n","    \"\"\"Creates an initial `TrainState`.\"\"\"\n","    params = module.init(rng, x)\n","    tx = optax.sgd(learning_rate, momentum)\n","    return RtxTrainState.create(apply_fn=module.apply, params=params, tx=tx)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
